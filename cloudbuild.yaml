steps:
  # Step-1: Clone the GitHub repository
  - name: 'gcr.io/cloud-builders/git'
    args: ['clone', 'https://github.com/jaiminbabariya7/dataproc-workflow-template.git']

  # Step-2: List the contents of the cloned repository to verify cloning
  - name: 'alpine'
    entrypoint: 'sh'
    args:
      - '-c'
      - 'ls -R /workspace/dataproc-workflow-template'

  # Step-3: Create Dataproc cluster
  #- name: 'gcr.io/cloud-builders/gcloud
    #args:
        #- 'dataproc'
        #- 'clusters'
        #- 'create'
        #- 'workflow-cluster'
        #- '--region'
        #- 'northamerica-northeast2'
        #- '--single-node'

  # Step-4: Submit Hadoop job to the cluster
  - name: 'gcr.io/cloud-builders/gcloud'
    args:
      - 'dataproc'
      - 'jobs'
      - 'submit'
      - 'hadoop'
      - '--cluster=workflow-cluster'
      - '--region=northamerica-northeast2'
      - '--jar=gs://workflow_buckt/target/hadoop-streaming.jar'
      - '--'
      - '-files=dataproc-workflow-template/wordcount.py'
      - '-mapper=python3 dataproc-workflow-template/wordcount.py'
      - '-reducer=python3 dataproc-workflow-template/wordcount.py'
      - '-input=gs://workflow_buckt/input/input.txt'
      - '-output=gs://workflow_buckt/output_data/'

  # Step-5: Delete the Dataproc cluster
  - name: 'gcr.io/cloud-builders/gcloud'
    args:
      - 'dataproc'
      - 'clusters'
      - 'delete'
      - 'workflow-cluster'
      - '--region=northamerica-northeast2'
      - '--quiet'

options:
  logging: CLOUD_LOGGING_ONLY
